{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735afc8c",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "Illustration: CNN - Convolutional Neural Network\n",
    "\n",
    "course: https://learn.deeplearning.ai/courses/pytorch-fundamentals/\n",
    "\n",
    "lesson: https://learn.deeplearning.ai/specializations/pytorch-for-deep-learning-professional-certificate/lesson/wrr1egt/cnns---part-1%3A-filters%2C-patterns%2C-and-feature-maps \n",
    "\n",
    "neural network concepts\n",
    "- computational graphs (static vs dynamic - GREAT): https://learn.deeplearning.ai/specializations/pytorch-for-deep-learning-professional-certificate/lesson/deeone4/dynamic-graphs \n",
    "- convolutional filters (cnn)\n",
    "- feature maps (cnn): output of a filtered image\n",
    "- convolve (cnn) - slide the filters over the image\n",
    "- the input image in a cnn is three-dimensional (RGB color channels, width, height)\n",
    "- pooling (cnn) - reduces size of feature map, for cost \n",
    "- regularization: dropout and batch normalization\n",
    "\n",
    "resources\n",
    "- training neural networks (stanford vid - GREAT explainer vid for intuition): https://www.youtube.com/watch?v=wEoyxE0GP2M\n",
    "- cnn basics (c321n stanford - GREAT VISUAL EXPLANATIONS for intuition)* : https://cs231n.github.io/convolutional-networks/\n",
    "- convnets walkthrough by Karpathy: https://www.youtube.com/watch?v=u6aEYuemt0M \n",
    "- wavenet to EEG brain data: https://arxiv.org/html/2510.15947v1 \n",
    "\n",
    "learning rates tuning\n",
    "File: \"Screenshot 2026-01-14 at 2.55.35â€¯PM.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b69f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # defines model architecture\n",
    "\n",
    "        # currently for dataset images of size 28x28, for variable image sizes, use adaptive pooling (ask LLM for illustration)\n",
    "\n",
    "        # super(SimpleCNN, self).__init__()\n",
    "        super().__init__()\n",
    "\n",
    "        # first convolutional block\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,  # in channel: 1 for grayscale value, 3 if RGB \n",
    "            out_channels=32,    # out channels are num of features (num of feature maps, each of an input image size if first layer)\n",
    "            kernel_size=3,  # filter size - kernel size of 3 means 3x3\n",
    "            padding=1   # padding a filter so can center near edges or corners of the input image\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()  # activation layer\n",
    "        self.pool1 = nn.MaxPool2d(  # pooling layer - for max pooling, max value of a feature map window is kept (2x2 window here if kernel_size=2), rest of feature map window is thrown away\n",
    "            kernel_size=2,\n",
    "            # stride=2    # stride slides or convolves n steps/pixels across the image\n",
    "        )\n",
    "\n",
    "        # second convolutional block\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, # in channel: 32 here since output from previous conv layer is 32\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()  # activation layer\n",
    "        self.pool2 = nn.MaxPool2d(  # pooling layer \n",
    "            kernel_size=2\n",
    "        )\n",
    "\n",
    "        # flatten layer \n",
    "        # before feeding into final fully connected layers\n",
    "        self.flatten = nn.Flatten() # flattens the 3 dimensional input into a vector for output\n",
    "\n",
    "        # -- fully connected layers --\n",
    "        flattened_size = 64 * 7 * 7 # flattened size: num of inputs (from output num of layer before flattening) * feature map size (7 x 7 here since initial image size is 28x28, halved after 2x2 pooling window in first layer to 14x14, then halved again to 7x7 here)\n",
    "        self.fc1 = nn.Linear(\n",
    "            flattened_size,\n",
    "            128 # output channel\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)  # regularization: dropout prevents overfitting and increases model robustness to other datasets by dropping percentage of neurons during training\n",
    "\n",
    "        # output layer\n",
    "        # classification layer\n",
    "        self.fc2_output = nn.Linear(\n",
    "            128,    # input size from output of previous layer\n",
    "            10    # output neurons - num of categories for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # defines data flow through model layers\n",
    "\n",
    "        # first conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # second conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # flattening layer\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # output layer - fully connected layer 2\n",
    "        x = self.fc2_output(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# create model instance\n",
    "model = SimpleCNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a4f0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Modularize for maintainability and reusability\n",
    "# refactor explicit architecture above into modular blocks\n",
    "# rely on nn.Sequential\n",
    "\n",
    "class SimpleCNNModular(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        define nn architecture with modular blocks\n",
    "        \"\"\"\n",
    "        \n",
    "        # currently for dataset images of size 28x28, for variable image sizes, use adaptive pooling (ask LLM for illustration)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(1, 32),\n",
    "            ConvBlock(32, 64)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        flattened_size = 64 * 7 * 7\n",
    "        classification_categories = 10\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(  # layer to process flattened layer - fully connected layer\n",
    "                flattened_size,\n",
    "                128\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(  # output layer - fully connected layer\n",
    "                128,\n",
    "                classification_categories\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(f\"forward pass\")\n",
    "\n",
    "        print(f\"NN shape upon input: {x.shape}\")\n",
    "        \n",
    "        x = self.features(x)\n",
    "        print(f\"NN shape after features extraction: {x.shape}\")\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        print(f\"NN shape after flattening: {x.shape}\")\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        print(f\"NN shape after classification: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            # nn.BatchNorm2d(out_channels), # batch normalization layer: stabilize to prevent unstable activations by normalizing each mini batch per feature channel (calculate mean and variance per batch, then normalize the batch)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2, \n",
    "                stride=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "# initialize model\n",
    "model = SimpleCNNModular()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ee30ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of parameters: 421642\n",
      "features.0.block.0.weight: torch.Size([32, 1, 3, 3])\n",
      "features.0.block.0.bias: torch.Size([32])\n",
      "features.1.block.0.weight: torch.Size([64, 32, 3, 3])\n",
      "features.1.block.0.bias: torch.Size([64])\n",
      "classifier.0.weight: torch.Size([128, 3136])\n",
      "classifier.0.bias: torch.Size([128])\n",
      "classifier.3.weight: torch.Size([10, 128])\n",
      "classifier.3.bias: torch.Size([10])\n",
      "features Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "flatten Flatten(start_dim=1, end_dim=-1)\n",
      "classifier Sequential(\n",
      "  (0): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "features Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.0 ConvBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "features.0.block Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "features.0.block.0 Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.0.block.1 ReLU()\n",
      "features.0.block.2 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "features.1 ConvBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "features.1.block Sequential(\n",
      "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "features.1.block.0 Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.1.block.1 ReLU()\n",
      "features.1.block.2 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "flatten Flatten(start_dim=1, end_dim=-1)\n",
      "classifier Sequential(\n",
      "  (0): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "classifier.0 Linear(in_features=3136, out_features=128, bias=True)\n",
      "classifier.1 ReLU()\n",
      "classifier.2 Dropout(p=0.5, inplace=False)\n",
      "classifier.3 Linear(in_features=128, out_features=10, bias=True)\n",
      "features.0.block.0.weight: torch.Size([32, 1, 3, 3])\n",
      "features.0.block.0.bias: torch.Size([32])\n",
      "features.1.block.0.weight: torch.Size([64, 32, 3, 3])\n",
      "features.1.block.0.bias: torch.Size([64])\n",
      "classifier.0.weight: torch.Size([128, 3136])\n",
      "classifier.0.bias: torch.Size([128])\n",
      "classifier.3.weight: torch.Size([10, 128])\n",
      "classifier.3.bias: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Model Inspection + Debugging\n",
    "# more advanced techniques in exercise 7 file \"pytorch-fundamentals-7-model-debugging.ipynb\"\n",
    "\n",
    "# understanding a layer's shape: torch.Size([output_neurons_size, input_neurons_size]) => think of matrix of size m x n => m for output neurons size, n for input neurons size  \n",
    "\n",
    "# How many parameters does the model have?\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f\"Total num of parameters: {total_params}\")\n",
    "\n",
    "# How can I see the shapes for each layer?\n",
    "for name, param in model.named_parameters():    # layer name => named parameters\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "\n",
    "# How can I see the shapes for nested blocks (modularized blocks as in conv blocks above via nn.Sequential())\n",
    "for name, module in model.named_children(): # only show top-level modules (children modules of root module)\n",
    "    print(name, module)\n",
    "\n",
    "for name, module in model.named_modules():  # shows all modules including nested sub-modules\n",
    "    if name:    # skips printing the model itself\n",
    "        print(name, module)\n",
    "\n",
    "\n",
    "# [Troubleshooting] RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x2048 and 1024x512)\n",
    "# shape mismatch between inputs to a linear layer\n",
    "# print the name and shape of the layers to see if there is a discrepancy with the shapes in the error message\n",
    "# also add debugging statements in the forward pass method within model via shape tracing\n",
    "for name, param in model.named_parameters():    # layer name => named parameters\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddacf737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
