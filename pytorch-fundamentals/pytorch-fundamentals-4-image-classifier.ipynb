{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60c115a",
   "metadata": {},
   "source": [
    "# Image Classifier\n",
    "\n",
    "course: https://learn.deeplearning.ai/courses/pytorch-fundamentals/\n",
    "\n",
    "lesson: https://learn.deeplearning.ai/specializations/pytorch-for-deep-learning-professional-certificate/lesson/shhwqe/image-classification---part-1\n",
    "\n",
    "Dataset: MNIST\n",
    "\n",
    "Purpose: Demonstrates a more mature training pipeline with everything learned so far in the course creating a PyTorch image classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c0d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "# if running via local Python runtime instead of Colab\n",
    "# addresses certificate error when retrieving from MNIST url, since local does not have SSL certificate correctly set up\n",
    "\n",
    "import os, ssl\n",
    "import certifi  # pip install certifi if needed\n",
    "\n",
    "# Point SSL to certifi's CA bundle\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "\n",
    "# Make default HTTPS context use this cert bundle\n",
    "ssl._create_default_https_context = ssl.create_default_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d742741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as torchvision_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b154074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Pre-processing\n",
    "\n",
    "# pre-processing: initialize functions to transform data\n",
    "mnist_dataset_mean = 0.1307 # value provided by course\n",
    "mnist_dataset_std = 0.3081  # value provided by course\n",
    "transform = torchvision_transforms.Compose([\n",
    "    torchvision_transforms.ToTensor(),  # converts data to tensor\n",
    "    torchvision_transforms.Normalize((mnist_dataset_mean,), (mnist_dataset_std,))\n",
    "])\n",
    "\n",
    "# load dataset: MNIST \n",
    "dataset_file_path = './data'    # if running via Colab, will be stored there\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dataset_file_path,\n",
    "    train=True, # retrieves 60,000 MNIST training images\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dataset_file_path,\n",
    "    train=False, # retrieves 10,000 MNIST test images\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# create data loaders to segment datasets into batches\n",
    "train_dataset_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True    # shuffles dataset for each epoch, since dataset may be sorted which would bias training through unintended patterns (e.g., all 0 digits in a batch since they are at top of dataset)\n",
    ")\n",
    "test_dataset_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1000,    # okay to have larger batch sizes since no need to calculate gradients, which may overwhelm memory\n",
    "    shuffle=False   # doesn't matter if data is sorted since just testing the trained model, not worried about biasing it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6236a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data: Analyze data\n",
    "\n",
    "# for my own understanding\n",
    "input_image_tensor, target_label = train_dataset[0]    # first sample\n",
    "input_image_tensor.shape   # out: [1, 28, 28]  => 3 features (grayscale, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ef0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training: Build Model\n",
    "\n",
    "# create neural network for MNIST classifier\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # flatten 2D images into a vector for linear nn\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(784, 128),    # each image is 28x28 pixels, so flattened vector would be 784 for linear layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)  # output is 10 digits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8e21b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Training: Initialize training objects\n",
    "\n",
    "# GPU acceleration if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: { device }')\n",
    "\n",
    "# Initialize model\n",
    "model = MNISTClassifier().to(device)    # move to device\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()   # loss function for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52f98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training: Define training function\n",
    "\n",
    "# define training function at epoch level for all batches\n",
    "def train_epoch(\n",
    "    model,\n",
    "    train_dataset_loader,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    device\n",
    "):\n",
    "    \n",
    "    # training mode for model\n",
    "    model.train()   \n",
    "    \n",
    "    # initialize variables: tracking progress\n",
    "    running_loss = 0.0  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataset_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)   # move data to device\n",
    "\n",
    "        optimizer.zero_grad()   # reset gradients for batch cycle\n",
    "        output = model(inputs)\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track progress\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)    # gets output label (e.g., digit) with highest probability of being correct (highest confidence score, lowest loss)\n",
    "\n",
    "        correct += predicted.eq(targets).sum().item()    # add up all correct outputs\n",
    "        total += targets.size(0) # add number of samples in batch to total\n",
    "\n",
    "        # Print progress\n",
    "        PROGRESS_EVERY_N_BATCH = 100\n",
    "        if batch_idx % PROGRESS_EVERY_N_BATCH == 0 and batch_idx > 0:\n",
    "            avg_loss = running_loss / 100\n",
    "            accuracy = 100. * correct / total\n",
    "            print(f'[{batch_idx * 64}/{ 60000 }]'    # batch size per n batches out of total training dataset samples\n",
    "                  f' '\n",
    "                  f'Loss: {avg_loss:.3f} | Accuracy: {accuracy:.1f}%')\n",
    "            \n",
    "            running_loss = 0.0  # resets running loss after every n batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6667008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Evaluation: Define evaluation function\n",
    "\n",
    "def evaluate(\n",
    "        model,\n",
    "        test_dataset_loader,\n",
    "        device\n",
    "):\n",
    "    \n",
    "    model.eval()    # set model to evaluation mode\n",
    "    \n",
    "    # initialize variables: tracking progress\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():       # no gradient descent since in eval mode, will speed things up\n",
    "        for inputs, targets in test_dataset_loader:\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Retrieve evaluation variables for accuracy score\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)     # add size of batch to total\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.1f}%')\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f616cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[6400/60000] Loss: 0.635 | Accuracy: 81.5%\n",
      "[12800/60000] Loss: 0.343 | Accuracy: 85.7%\n",
      "[19200/60000] Loss: 0.255 | Accuracy: 87.9%\n",
      "[25600/60000] Loss: 0.235 | Accuracy: 89.2%\n",
      "[32000/60000] Loss: 0.235 | Accuracy: 90.0%\n",
      "[38400/60000] Loss: 0.184 | Accuracy: 90.8%\n",
      "[44800/60000] Loss: 0.169 | Accuracy: 91.4%\n",
      "[51200/60000] Loss: 0.167 | Accuracy: 91.9%\n",
      "[57600/60000] Loss: 0.151 | Accuracy: 92.3%\n",
      "Test Accuracy: 95.8%\n",
      "Epoch: 2\n",
      "[6400/60000] Loss: 0.134 | Accuracy: 96.2%\n",
      "[12800/60000] Loss: 0.109 | Accuracy: 96.6%\n",
      "[19200/60000] Loss: 0.105 | Accuracy: 96.7%\n",
      "[25600/60000] Loss: 0.122 | Accuracy: 96.6%\n",
      "[32000/60000] Loss: 0.113 | Accuracy: 96.6%\n",
      "[38400/60000] Loss: 0.120 | Accuracy: 96.5%\n",
      "[44800/60000] Loss: 0.106 | Accuracy: 96.6%\n",
      "[51200/60000] Loss: 0.107 | Accuracy: 96.6%\n",
      "[57600/60000] Loss: 0.103 | Accuracy: 96.7%\n",
      "Test Accuracy: 97.1%\n",
      "Epoch: 3\n",
      "[6400/60000] Loss: 0.078 | Accuracy: 97.7%\n",
      "[12800/60000] Loss: 0.076 | Accuracy: 97.7%\n",
      "[19200/60000] Loss: 0.077 | Accuracy: 97.7%\n",
      "[25600/60000] Loss: 0.082 | Accuracy: 97.6%\n",
      "[32000/60000] Loss: 0.091 | Accuracy: 97.5%\n",
      "[38400/60000] Loss: 0.084 | Accuracy: 97.5%\n",
      "[44800/60000] Loss: 0.080 | Accuracy: 97.5%\n",
      "[51200/60000] Loss: 0.071 | Accuracy: 97.5%\n",
      "[57600/60000] Loss: 0.083 | Accuracy: 97.5%\n",
      "Test Accuracy: 97.2%\n",
      "Epoch: 4\n",
      "[6400/60000] Loss: 0.058 | Accuracy: 98.2%\n",
      "[12800/60000] Loss: 0.057 | Accuracy: 98.2%\n",
      "[19200/60000] Loss: 0.057 | Accuracy: 98.2%\n",
      "[25600/60000] Loss: 0.060 | Accuracy: 98.1%\n",
      "[32000/60000] Loss: 0.057 | Accuracy: 98.1%\n",
      "[38400/60000] Loss: 0.072 | Accuracy: 98.1%\n",
      "[44800/60000] Loss: 0.067 | Accuracy: 98.0%\n",
      "[51200/60000] Loss: 0.059 | Accuracy: 98.0%\n",
      "[57600/60000] Loss: 0.053 | Accuracy: 98.1%\n",
      "Test Accuracy: 97.7%\n",
      "Epoch: 5\n",
      "[6400/60000] Loss: 0.043 | Accuracy: 98.7%\n",
      "[12800/60000] Loss: 0.044 | Accuracy: 98.6%\n",
      "[19200/60000] Loss: 0.045 | Accuracy: 98.6%\n",
      "[25600/60000] Loss: 0.044 | Accuracy: 98.6%\n",
      "[32000/60000] Loss: 0.048 | Accuracy: 98.6%\n",
      "[38400/60000] Loss: 0.052 | Accuracy: 98.5%\n",
      "[44800/60000] Loss: 0.051 | Accuracy: 98.5%\n",
      "[51200/60000] Loss: 0.052 | Accuracy: 98.5%\n",
      "[57600/60000] Loss: 0.056 | Accuracy: 98.4%\n",
      "Test Accuracy: 97.4%\n",
      "Epoch: 6\n",
      "[6400/60000] Loss: 0.036 | Accuracy: 98.8%\n",
      "[12800/60000] Loss: 0.040 | Accuracy: 98.8%\n",
      "[19200/60000] Loss: 0.035 | Accuracy: 98.8%\n",
      "[25600/60000] Loss: 0.041 | Accuracy: 98.8%\n",
      "[32000/60000] Loss: 0.040 | Accuracy: 98.8%\n",
      "[38400/60000] Loss: 0.030 | Accuracy: 98.8%\n",
      "[44800/60000] Loss: 0.037 | Accuracy: 98.8%\n",
      "[51200/60000] Loss: 0.045 | Accuracy: 98.8%\n",
      "[57600/60000] Loss: 0.051 | Accuracy: 98.7%\n",
      "Test Accuracy: 97.2%\n",
      "Epoch: 7\n",
      "[6400/60000] Loss: 0.027 | Accuracy: 99.2%\n",
      "[12800/60000] Loss: 0.023 | Accuracy: 99.2%\n",
      "[19200/60000] Loss: 0.030 | Accuracy: 99.2%\n",
      "[25600/60000] Loss: 0.027 | Accuracy: 99.2%\n",
      "[32000/60000] Loss: 0.031 | Accuracy: 99.2%\n",
      "[38400/60000] Loss: 0.030 | Accuracy: 99.1%\n",
      "[44800/60000] Loss: 0.041 | Accuracy: 99.1%\n",
      "[51200/60000] Loss: 0.036 | Accuracy: 99.0%\n",
      "[57600/60000] Loss: 0.038 | Accuracy: 99.0%\n",
      "Test Accuracy: 97.3%\n",
      "Epoch: 8\n",
      "[6400/60000] Loss: 0.025 | Accuracy: 99.3%\n",
      "[12800/60000] Loss: 0.025 | Accuracy: 99.2%\n",
      "[19200/60000] Loss: 0.022 | Accuracy: 99.2%\n",
      "[25600/60000] Loss: 0.027 | Accuracy: 99.2%\n",
      "[32000/60000] Loss: 0.022 | Accuracy: 99.2%\n",
      "[38400/60000] Loss: 0.023 | Accuracy: 99.2%\n",
      "[44800/60000] Loss: 0.030 | Accuracy: 99.2%\n",
      "[51200/60000] Loss: 0.036 | Accuracy: 99.2%\n",
      "[57600/60000] Loss: 0.037 | Accuracy: 99.1%\n",
      "Test Accuracy: 97.7%\n",
      "Epoch: 9\n",
      "[6400/60000] Loss: 0.020 | Accuracy: 99.4%\n",
      "[12800/60000] Loss: 0.019 | Accuracy: 99.4%\n",
      "[19200/60000] Loss: 0.022 | Accuracy: 99.4%\n",
      "[25600/60000] Loss: 0.021 | Accuracy: 99.4%\n",
      "[32000/60000] Loss: 0.020 | Accuracy: 99.4%\n",
      "[38400/60000] Loss: 0.023 | Accuracy: 99.4%\n",
      "[44800/60000] Loss: 0.028 | Accuracy: 99.3%\n",
      "[51200/60000] Loss: 0.027 | Accuracy: 99.3%\n",
      "[57600/60000] Loss: 0.020 | Accuracy: 99.3%\n",
      "Test Accuracy: 97.5%\n",
      "Epoch: 10\n",
      "[6400/60000] Loss: 0.019 | Accuracy: 99.4%\n",
      "[12800/60000] Loss: 0.016 | Accuracy: 99.5%\n",
      "[19200/60000] Loss: 0.016 | Accuracy: 99.4%\n",
      "[25600/60000] Loss: 0.021 | Accuracy: 99.4%\n",
      "[32000/60000] Loss: 0.026 | Accuracy: 99.3%\n",
      "[38400/60000] Loss: 0.020 | Accuracy: 99.3%\n",
      "[44800/60000] Loss: 0.027 | Accuracy: 99.3%\n",
      "[51200/60000] Loss: 0.022 | Accuracy: 99.3%\n",
      "[57600/60000] Loss: 0.023 | Accuracy: 99.3%\n",
      "Test Accuracy: 97.5%\n"
     ]
    }
   ],
   "source": [
    "# Training: Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: { epoch + 1 }')\n",
    "    train_epoch(\n",
    "        model,\n",
    "        train_dataset_loader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device\n",
    "    )\n",
    "    accuracy = evaluate(\n",
    "        model,\n",
    "        test_dataset_loader,\n",
    "        device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b170376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target label: 1\n",
      "predicted_outputs: tensor([[-11.2152,   7.5163,  -4.7328, -10.3949,  -5.9403,  -6.5737,  -3.2017,\n",
      "          -4.5079,  -2.5509, -10.8662]])\n",
      "predicted label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "# get input data\n",
    "input_image_tensor, target_label = test_dataset[2]\n",
    "# print(input_image_tensor)\n",
    "print(f'target label: {target_label}')\n",
    "\n",
    "# define inference function\n",
    "def inference_digit(input_image_tensor):\n",
    "    input_image_tensor = input_image_tensor.unsqueeze(0)  # since models expect batch, add dimension at index 0 for batch from [1,28,28] to [1,1,28,28]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_outputs = model(input_image_tensor)  # vector of confidence scores for all digit labels 0-9\n",
    "        print(f'predicted_outputs: {predicted_outputs}')\n",
    "        confidence_scores, predicted_label_tensor = predicted_outputs.max(1)\n",
    "        predicted_label = predicted_label_tensor.item()\n",
    "        # print(f'predicted_output {predicted_outputs}')    \n",
    "\n",
    "    print(f'predicted label: {predicted_label}')\n",
    "    return predicted_label\n",
    "    \n",
    "\n",
    "\n",
    "# run inference\n",
    "inference_digit(input_image_tensor)\n",
    "\n",
    "# visualize image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(input_image_tensor.squeeze())   # reduce image back to three feature dimensions by removing batch dimension at index 0\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c739e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "predicted_outputs: tensor([[ -8.1590,  12.0817,  -9.0322,   1.2620, -15.7547,  -6.3961, -25.3064,\n",
      "          -0.4236, -29.9919, -43.0300]])\n",
      "predicted label: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference: Own handwritten digit\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# load data\n",
    "# make sure image is pixel size 28x28\n",
    "handwritten_digit_image_path = 'data/custom/handwritten-digit-a.jpeg'   \n",
    "img_object = Image.open(handwritten_digit_image_path).convert(\"L\")    # converts image to grayscale (convert L)\n",
    "# print(img_object)\n",
    "\n",
    "# wrangle data\n",
    "img_tensor = transform(img_object)          # data processing: convert into tensor and normalize \n",
    "# img_tensor = img_tensor.unsqueeze(0)        # add batch dimension\n",
    "# print(img_tensor.shape)   # make sure image is pixel size 28x28 for correct shape => (1, 1, 28, 28)\n",
    "\n",
    "# inference\n",
    "inference_digit(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2da5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
