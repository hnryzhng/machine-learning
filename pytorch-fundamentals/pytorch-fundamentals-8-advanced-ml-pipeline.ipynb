{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39f9583",
   "metadata": {},
   "source": [
    "# Advanced ML Pipeline\n",
    "\n",
    "Putting it all together from preceding lessons and labs\n",
    "\n",
    "Dataset (CIFAR-10): https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image   # Python package: Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Inspection\n",
    "\n",
    "# play around with just one file to understand how to load and process the data\n",
    "\n",
    "file_path = './data/CIFAR-10-batches-py/data_batch_1'   # batch 1\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    # batch_data_dict = pickle.load(f, encoding='bytes') # python dict keys would be binary and inaccessible using natural language\n",
    "    batch_data_dict = pickle.load(f, encoding='latin1')\n",
    "print(batch_data_dict)\n",
    "\n",
    "for key in batch_data_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Access: Define Dataset class\n",
    "\n",
    "# Dataset: CIFAR-10\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    CIFAR-10 dataset from downloaded batch files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, train=True, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            raise FileNotFoundError(f\"CIFAR-10 directory not found: {self.root_dir}\")\n",
    "\n",
    "        self.data, self.labels = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        images_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        batch_files = (\n",
    "            [f\"data_batch_{i}\" for i in range(1, 6)]\n",
    "            if self.train\n",
    "            else [\"test_batch\"]\n",
    "        )\n",
    "\n",
    "        for batch_file in batch_files:\n",
    "            path = os.path.join(self.root_dir, batch_file)\n",
    "            images, labels = self._load_batch(path)\n",
    "            images_list.append(images)\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "        images = np.concatenate(images_list, axis=0)  # (N, 3, 32, 32)\n",
    "        labels = np.array(labels_list, dtype=np.int64)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def _load_batch(self, file_path):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            batch = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "        images = batch[\"data\"]          # (10000, 3072)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        images = images.reshape(-1, 3, 32, 32)\n",
    "        return images, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert (C, H, W) NumPy -> (H, W, C) for PIL\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "# put in own file for future usage\n",
    "\n",
    "# def denormalize(img_denorm, mean_list=[0.485, 0.456, 0.406], std_list=[0.229, 0.224, 0.225]):\n",
    "\n",
    "#     # @param [] mean_list: calculated list of means for the dataset such as [0.485, 0.456, 0.406] for OxfordFlowers\n",
    "#     # @param [] std_list: calculated list of standard deviations for the dataset such as [0.229, 0.224, 0.225] for OxfordFlowers\n",
    "\n",
    "#     mean = torch.tensor(mean_list).view(3, 1, 1)\n",
    "#     std = torch.tensor(std_list).view(3, 1, 1)\n",
    "\n",
    "#     img_denorm = img_denorm * std + mean\n",
    "#     img_denorm = img_denorm.clamp(0, 1)\n",
    "\n",
    "#     return img_denorm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def denormalize(img, mean_list=[0.485, 0.456, 0.406], std_list=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - torch.Tensor in CHW (preferred), OR\n",
    "      - np.ndarray in CHW/HWC\n",
    "\n",
    "    Returns:\n",
    "      - torch.Tensor in CHW in [0,1]\n",
    "    \"\"\"\n",
    "    # convert numpy -> torch\n",
    "    if isinstance(img, np.ndarray):\n",
    "        x = torch.from_numpy(img)\n",
    "        # if HWC, convert to CHW\n",
    "        if x.ndim == 3 and x.shape[-1] in (1, 3) and x.shape[0] not in (1, 3):\n",
    "            x = x.permute(2, 0, 1)\n",
    "        img = x.float()\n",
    "    elif torch.is_tensor(img):\n",
    "        img = img.float()\n",
    "    else:\n",
    "        raise TypeError(f\"denormalize expects torch.Tensor or np.ndarray, got {type(img)}\")\n",
    "\n",
    "    # ensure CHW\n",
    "    if img.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D image (C,H,W) or (H,W,C), got shape {tuple(img.shape)}\")\n",
    "\n",
    "    c = img.shape[0] if img.shape[0] in (1, 3) else img.shape[-1]\n",
    "    if img.shape[0] not in (1, 3) and img.shape[-1] in (1, 3):\n",
    "        img = img.permute(2, 0, 1)  # HWC -> CHW\n",
    "\n",
    "    mean = torch.tensor(mean_list, dtype=img.dtype, device=img.device).view(3, 1, 1)\n",
    "    std = torch.tensor(std_list, dtype=img.dtype, device=img.device).view(3, 1, 1)\n",
    "\n",
    "    out = img * std + mean\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "def visualize_raw_image(raw_image_array):\n",
    "    \n",
    "    # @param <np.array> raw image input array from dataset before any processing\n",
    "\n",
    "    img_reshaped = np.reshape(raw_image_array, (3, 32, 32))   # reshape to (C, H, W)\n",
    "    img_permuted = np.transpose(img_reshaped, (1, 2, 0))  # convert to (H, W, C) for matplotlib\n",
    "\n",
    "    plt.imshow(img_permuted)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # or visualize just raw image (not tensor) using Pillow\n",
    "\n",
    "\n",
    "def visualize_processed_image(loader_data, mean_list=(0.485,0.456,0.406), std_list=(0.229,0.224,0.225)):\n",
    "    \n",
    "    # @param DataLoader loader_data: data loader obj of dataset such as training loader data \n",
    "    \n",
    "    images, labels = next(iter(loader_data))\n",
    "    processed_img = images[0].cpu()\n",
    "\n",
    "    denorm = denormalize(processed_img, mean_list, std_list)   # CHW torch\n",
    "    img_hwc = denorm.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img_hwc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_data_augmentations(dataset, idx=0, num_versions=8, mean_list=(0.485,0.456,0.406), std_list=(0.229,0.224,0.225)):\n",
    "    \"\"\" See what data augmentations look like for a sample image \"\"\"\n",
    "    # @param Tensor dataset: just raw dataset tensor, not passed into DataLoader\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_versions):\n",
    "        img, label = dataset[idx]  # could be np or torch depending on your dataset\n",
    "\n",
    "        img = denormalize(img, mean_list, std_list)  # CHW torch\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing: Define Transformations\n",
    "\n",
    "# define transformations for each split dataset\n",
    "\n",
    "transform_training_fn = transforms.Compose([\n",
    "    # data augmentations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # standard preprocessing\n",
    "    # transforms.Resize(224), # since CIFAR-10 image is 32x32, no need to make smaller\n",
    "    # transforms.CenterCrop(200), # \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], # TODO: create util to calc mean and std\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_validation_fn = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(200),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], # TODO: create util to calc mean and std\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51519716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Access: Sample Inspection\n",
    "\n",
    "# inspect what transforms to apply to raw dataset\n",
    "\n",
    "root_dir = './data/CIFAR-10-batches-py'\n",
    "train_raw_dataset = CIFAR10Dataset(\n",
    "    root_dir,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "RAW_IMAGE_INDEX = 0\n",
    "raw_image_np_array = train_raw_dataset.data[RAW_IMAGE_INDEX]    # train dataset input images already transformed into tensors\n",
    "visualize_raw_image(raw_image_np_array)\n",
    "\n",
    "\n",
    "# Play around with which transformations or processing to apply (e.g., resizing, cropping, convert to tensors, etc.)\n",
    "\n",
    "img = train_raw_dataset.data[0] # raw image np array\n",
    "label = train_raw_dataset.labels[0]\n",
    "img_resized = transforms.Resize(256)(img)\n",
    "img_resized_cropped = transforms.CenterCrop(224)(img_resized)\n",
    "img_tensor = transforms.ToTensor()(img_resized_cropped)\n",
    "\n",
    "print(img_resized.size)\n",
    "\n",
    "# print(img_resized.show())  # TODO: visualize raw image using Pillow\n",
    "visualize_raw_image(img)\n",
    "\n",
    "print(img_tensor)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data PreProcessing: Create actual split transformed datasets\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 0\n",
    "TRAIN_RATIO = 0.8\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# Base datasets (same data, different transforms)\n",
    "# full train dataset will be split into train and val subsets, since test dataset is already provided for CIFAR-10\n",
    "full_train_dataset = CIFAR10Dataset(\n",
    "    root_dir=root_dir,\n",
    "    train=True,\n",
    "    transform=None  # transform applied in __getitem__ if to be added\n",
    ")   # will split into train and validation subsets\n",
    "\n",
    "test_dataset = CIFAR10Dataset(\n",
    "    root_dir=root_dir,\n",
    "    train=False,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "dataset_size = len(full_train_dataset)\n",
    "train_size = int(TRAIN_RATIO * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "print(f\"Train samples: {train_size}\")\n",
    "print(f\"Validation samples: {val_size}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "# Create index split\n",
    "# only splitting train and validation sets since test set exists already for CIFAR-10\n",
    "indices = torch.randperm(dataset_size)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "\n",
    "# Subsets\n",
    "# only splitting train and validation sets since test set exists already for CIFAR-10\n",
    "train_subset = Subset(full_train_dataset, train_indices)\n",
    "val_subset = Subset(full_train_dataset, val_indices)\n",
    "\n",
    "# Optional: wrap subsets to apply different transforms\n",
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_dataset = CIFAR10Dataset(\n",
    "    root_dir=root_dir,\n",
    "    train=True,\n",
    "    transform=transform_training_fn\n",
    ")\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "val_dataset = CIFAR10Dataset(\n",
    "    root_dir=root_dir,\n",
    "    train=True,\n",
    "    transform=transform_validation_fn\n",
    ")\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "test_dataset = TransformSubset(\n",
    "    test_dataset, \n",
    "    transform=transform_validation_fn\n",
    ")\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# Basic model\n",
    "\n",
    "\n",
    "class CNNBasic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # input image size is 32 x 32 for dataset CIFAR-10\n",
    "        # image size = 32\n",
    "\n",
    "        # conv block 1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,    # input channel = color channel (grayscale is 1, RGB is 3)\n",
    "            out_channels=32,  # how many feature maps to produce\n",
    "            kernel_size=3,   # filter size\n",
    "            padding=1,  # filter padding\n",
    "        )\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "\n",
    "        # conv block 2\n",
    "        # image size is halved due to preceding pooling\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,  # how many feature maps to produce\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # conv block 3\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,  # how many feature maps to produce\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.activation3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # fully connected blocks\n",
    "        flattened_size = 128 * 4 * 4    # prev layer's output channels * input tensor size after latest pooling\n",
    "        fc_output = 256\n",
    "        self.fc1 = nn.Linear(\n",
    "            flattened_size,\n",
    "            fc_output\n",
    "        )\n",
    "        self.activation_output = nn.ReLU()\n",
    "        \n",
    "        # output layer\n",
    "        output_classes = 10\n",
    "        final_layer_input = fc_output\n",
    "        self.fc_output = nn.Linear(\n",
    "            final_layer_input,\n",
    "            output_classes\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x) \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # conv block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # conv block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "\n",
    "        # flatten\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # fully connected blocks\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_output(x)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.fc_output(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ede77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inspection\n",
    "\n",
    "# get data batch for testing model\n",
    "# img_batch, label_batch = next(iter(train_loader))\n",
    "# print(\"Batch shape:\", img_batch.shape)  # Should be [batch_size, 1, 28, 28] - color channel grayscale, image size 28x28\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "img_batch, label_batch = batch\n",
    "print('img batch size: ', img_batch.size())\n",
    "\n",
    "# initialize model on data batch\n",
    "cnn_basic = CNNBasic()\n",
    "cnn_basic(img_batch)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBasicDebug(CNNBasic):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        print(\n",
    "            \"(Layer components) Conv layer 1 parameters (weights, biases) shapes:\",\n",
    "            self.conv1.weight.shape,\n",
    "            self.conv1.bias.shape\n",
    "        )\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        \n",
    "        print(\"(Activation) Tensor shape after convolution 1 and ReLU activation:\", x.shape)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        print(\"(Activation) Tensor shape after pooling 1:\", x.shape)\n",
    "        \n",
    "        print(\"---\")\n",
    "\n",
    "        print(\n",
    "            \"(Layer components) Conv layer 2 parameters (weights, biases) shapes:\",\n",
    "            self.conv2.weight.shape,\n",
    "            self.conv2.bias.shape\n",
    "        )\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        \n",
    "        print(\"(Activation) Tensor shape after convolution 2 and ReLU activation:\", x.shape)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        print(\"(Activation) Tensor shape after pooling 2: \", x.shape)\n",
    "\n",
    "        print(\"---\")\n",
    "\n",
    "        print(\n",
    "            \"(Layer components) Conv layer 3 parameters (weights, biases) shapes:\",\n",
    "            self.conv3.weight.shape,\n",
    "            self.conv3.bias.shape\n",
    "        )\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        \n",
    "        print(\"(Activation) Tensor shape after convolution 3 and ReLU activation:\", x.shape)\n",
    "        \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        print(\"(Activation) Tensor shape after pooling 3:\", x.shape)\n",
    "\n",
    "        # Note: Addresses shape mismatch error between conv block and fully connected block by flattening the layer\n",
    "\n",
    "        x = torch.flatten(\n",
    "            x, start_dim=1\n",
    "        )  # Flatten all dimensions except batch\n",
    "        \n",
    "        print(\"(Activation) After flattening:\", x.shape)\n",
    "        \n",
    "        print(\"---\")\n",
    "\n",
    "        print(\n",
    "            \"(Layer components) Linear layer fc1\",\n",
    "            self.fc1.weight.shape,\n",
    "            self.fc1.bias.shape\n",
    "        )\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_output(x)\n",
    "\n",
    "        print(\"(Activation) Tensor shape after fc1 and ReLU:\", x.shape)\n",
    "        \n",
    "        print(\"---\")\n",
    "        \n",
    "        print(\n",
    "            \"(Layer components) Linear layer fc2 parameters (weights, biases):\",\n",
    "            self.fc_output.weight.shape,\n",
    "            self.fc_output.bias.shape,\n",
    "        )\n",
    "        x = self.fc_output(x)\n",
    "\n",
    "\n",
    "        print(\"(Activation) After fc2 (output):\", x.shape)\n",
    "\n",
    "        print(\"---\")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7545ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model debug\n",
    "\n",
    "cnn_basic_debug = CNNBasicDebug()\n",
    "\n",
    "cnn_basic_debug(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Modularize\n",
    "\n",
    "# create reusable convolutional block (conv layer, activation layer, pooling layer)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        conv_kernel_size=3,\n",
    "        conv_padding=1,\n",
    "        pooling_kernel_size=2,\n",
    "        pooling_stride=2,\n",
    "        use_batchnorm=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_padding\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        layers.append(\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=pooling_kernel_size,\n",
    "                stride=pooling_stride\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Modularize\n",
    "\n",
    "\n",
    "class CNNModular(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.convblock1 = ConvBlock(3, 32)\n",
    "        self.convblock2 = ConvBlock(32, 64)\n",
    "        self.convblock3 = ConvBlock(64, 128)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        flattened_size = 128 * 4 * 4    # prev layer's output channels * input tensor size after latest pooling\n",
    "        fc_output = 256\n",
    "        self.fc1 = nn.Linear(flattened_size, fc_output)\n",
    "\n",
    "        output_classes = 10\n",
    "        final_layer_input = fc_output\n",
    "        self.fc_out = nn.Linear(final_layer_input, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Modularize\n",
    "\n",
    "cnn_modular = CNNModular()\n",
    "model = cnn_modular(img_batch)\n",
    "\n",
    "print(\"Output shape from modularized sequential model:\", model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Analysis\n",
    "\n",
    "# Debug\n",
    "\n",
    "# TODO: show activation statistics to see flow of information\n",
    "\n",
    "# TODO: create utils to visualize activation stats of layers via histograms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40752b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "# Model Evaluation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
