{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39f9583",
   "metadata": {},
   "source": [
    "# Advanced ML Pipeline\n",
    "\n",
    "Putting it all together from preceding lessons and labs\n",
    "\n",
    "Dataset (CIFAR-10): https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "230a2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26e5d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_label\n",
      "labels\n",
      "data\n",
      "filenames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/cws687_10tb7d_q4kvb0d8t40000gn/T/ipykernel_67208/1544385664.py:8: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  batch_data_dict = pickle.load(f, encoding='latin1')\n"
     ]
    }
   ],
   "source": [
    "# Data inspection\n",
    "\n",
    "# play around with just one file to understand how to load and process the data\n",
    "\n",
    "file_path = './data/CIFAR-10-batches-py/data_batch_1'   # batch 1\n",
    "with open(file_path, 'rb') as f:\n",
    "    # batch_data_dict = pickle.load(f, encoding='bytes') # python dict keys would be binary and inaccessible using natural language\n",
    "    batch_data_dict = pickle.load(f, encoding='latin1')\n",
    "# print(batch_data_dict)\n",
    "\n",
    "for key in batch_data_dict:\n",
    "    print(key)\n",
    "\n",
    "# batch_data_dict['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c171aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Access\n",
    "\n",
    "# Dataset: CIFAR-10\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "\n",
    "    '''\n",
    "    CIFAR-10 Python dataset of downloaded blobs in 5 batches\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir, train=True, transform=None):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            raise FileNotFoundError(f\"CIFAR-10 directory not found: {self.root_dir}\")\n",
    "        \n",
    "        self.data, self.labels = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        images_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        num_batches = 5\n",
    "\n",
    "        if self.train:\n",
    "            batch_file_names = [f\"data_batch_{i}\" for i in range(1, num_batches+1)]\n",
    "        else:\n",
    "            batch_file_names = [\"test_batch\"]\n",
    "\n",
    "        for batch_file_name in batch_file_names:\n",
    "            path = os.path.join(self.root_dir, batch_file_name)\n",
    "            images, labels = self._load_batch(path)\n",
    "            images_list.append(images)\n",
    "            labels_list.extend(labels)\n",
    "        \n",
    "        images = np.concatenate(images_list, axis=0)    # convert all image batches into one array - TODO NECESSARY?\n",
    "        labels = np.array(labels_list, dtype=np.float32)    # TODO: or int64?\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def _load_batch(self, file_path):\n",
    "        \n",
    "        \"\"\" Load a single batch from corresponding file \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            batch_data_dict = pickle.load(f, encoding='latin1')   # TODO: right encoding? bytes (in cifar10 docs), latin1 (llm suggested)\n",
    "        \n",
    "        images = batch_data_dict['data']  # shape: [10000, 3072] since docs say has 10000 x 3072 numpy array of uint8s per batch\n",
    "        labels = batch_data_dict['labels']    # length: 10000\n",
    "\n",
    "        # Reshape to (N, 3, 32, 32) array\n",
    "        images = np.reshape(images, (-1, 3, 32, 32))\n",
    "        \n",
    "        return images, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # convert to float tensor?\n",
    "\n",
    "        # optional individual image transforms\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5020690c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (728913531.py, line 12)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtransforms.ToTensor()\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# define transformations for each split dataset\n",
    "\n",
    "transform_training_fn = transforms.Compose([\n",
    "    # data augmentations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # standard preprocessing\n",
    "    transform.Resize(),\n",
    "    transformsCenterCrop(),\n",
    "    transforms.ToTensor()\n",
    "    transforms.Normalize(\n",
    "        mean=[],    # TODO: util to calc mean and stds of dataset\n",
    "        std=[]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_validation_fn = transforms.Compose([\n",
    "    transform.Resize(),\n",
    "    transformsCenterCrop(),\n",
    "    transforms.ToTensor()\n",
    "    transforms.Normalize(\n",
    "        mean=[],    # TODO: util to calc mean and stds of dataset \n",
    "        std=[]\n",
    "    )\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "# put in own file for future usage\n",
    "\n",
    "def denormalize(img_denorm, mean_list=[0.485, 0.456, 0.406], std_list=[0.229, 0.224, 0.225]):\n",
    "\n",
    "    # @param [] mean_list: calculated list of means for the dataset such as [0.485, 0.456, 0.406] for OxfordFlowers\n",
    "    # @param [] std_list: calculated list of standard deviations for the dataset such as [0.229, 0.224, 0.225] for OxfordFlowers\n",
    "\n",
    "    mean = torch.tensor(mean_list).view(3, 1, 1)\n",
    "    std = torch.tensor(std_list).view(3, 1, 1)\n",
    "\n",
    "    img_denorm = img_denorm * std + mean\n",
    "    img_denorm = img_denorm.clamp(0, 1)\n",
    "\n",
    "    return img_denorm\n",
    "\n",
    "def visualize_processed_image(loader_data):\n",
    "\n",
    "    # @param DataLoader loader_data: data loader obj of dataset such as training loader data \n",
    "\n",
    "    images, labels = next(iter(loader_data))   # gets one batch\n",
    "\n",
    "    processed_img_tensor = images[0].cpu()    # move image to cpu from gpu if needed for further operations\n",
    "    denormalized_img_tensor = denormalize(processed_img_tensor) # denormalize image first\n",
    "\n",
    "    # visualize processed image\n",
    "    plt.imshow(denormalized_img_tensor.permute(1, 2, 0))  # convert C,H,W (channel, heigth, width) -> H,W,C since PyTorch tensors are [C, H, W], but matplotlib expects [H, W, C]:\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_data_augmentations(dataset, idx=0, num_versions=8):\n",
    "    \"\"\" See what data augmentations look like for a sample image \"\"\"\n",
    "    # @param Tensor dataset: just raw dataset tensor, not passed into DataLoader\n",
    "\n",
    "    fix, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_versions):\n",
    "        img, label = dataset[idx]   # retrieve augmented version\n",
    "\n",
    "        # denormalize for display\n",
    "        img = denormalize(img)\n",
    "\n",
    "        axes[i].imshow(img.permute(1, 2, 0))  # CHW to HWC\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51519716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/cws687_10tb7d_q4kvb0d8t40000gn/T/ipykernel_67208/1707661616.py:49: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  batch_data_dict = pickle.load(f, encoding='latin1')   # TODO: right encoding? bytes (in cifar10 docs), latin1 (llm suggested)\n"
     ]
    }
   ],
   "source": [
    "# Data Access\n",
    "\n",
    "# sanity check by testing transforms on raw dataset and loaded dataset\n",
    "\n",
    "root_dir = './data/CIFAR-10-batches-py'\n",
    "train_dataset = CIFAR10Dataset(\n",
    "    root_dir,\n",
    "    train=True\n",
    ")\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "# visualize_processed_image(train_data_loader)\n",
    "# visualize_data_augmentations(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e20678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35749c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# Basic model\n",
    "\n",
    "\n",
    "class NNBasic(nn.Module):\n",
    "    \n",
    "    # go with CNN\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # input image size is ??\n",
    "        # image_size = ??\n",
    "\n",
    "        # conv block 1\n",
    "        self.conv1 = self.Conv2d(\n",
    "            in_channels=1,    # input channel = color channel (grayscale is 1, RGB is 3)\n",
    "            out_channels=32,  # how many feature maps to produce\n",
    "            kernel_size=3,   # filter size\n",
    "            padding=1,  # filter padding\n",
    "        )\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "\n",
    "        # conv block 2\n",
    "        # image size is halved due to preceding pooling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # fully connected blocks\n",
    "        flattened_size = 0  # ??\n",
    "        self.fc1 = nn.Linear(\n",
    "            flattened_size,\n",
    "            128\n",
    "        )\n",
    "\n",
    "        # output layer\n",
    "        output_classes = 10\n",
    "        # self.fc_output = nn.Linear(\n",
    "\n",
    "        # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ede77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# Inspection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
